[ Web Crawling ]
    1) 정의 : 인터넷 웹 페이지에서 원하는 데이터(html)을 수집하는 기술

    2) 목적 :
        - 데이터 수집 : ex: 뉴스, 상풍, 날씨, 위치 등등
        - 데이터 분석 : ex: 자료 수집 > 저장 > 통계 > 학습(AI)
        - 업무 효율성 : ex: 클릭이벤트, 로그인

    3) 웹 페이지의 구동
        (1) 정적 페이지
            - HTML 소스의 데이터가 있는 경우
            - ex) 게시물의 정보가 포함된 html
            -Jsoup 라이브러리
        (2) 동적 페이지
            - JS가 HTML 소스에 데이터를 넣어주는 경우
            - ex) 게시물의 정보가 없는 html >> js가 fetch로 백엔드에서 정보를 가져와 html에 데이터를 출력
            - Selenium 라이브러리

    4) robots.txt
        - 웹사이트 루트 경로에 있는 자동화(크롤링) 접근 규칙 파일
        1) 웹페이지 URL + "/robots.txt"
        2) allow    : 접근 허용
           disallow : 접근 차단
        - 웹크롤링 한 번이 REST 요청 1번으로 데이터베이스 작동 >> 크롤링으로 인한 과부하를 발생시킬 수 있음

    5) 사용법

[ Jsoup ]
    1) 설치
        https://mvnrepository.com/artifact/org.jsoup/jsoup
        Gradle 설치
        ` implementation("org.jsoup:jsoup:1.21.2") `
    2) 주요 메소드
        (1) ` Document document = Jsoup.connect(URL).get(); `
            - 주의 : import org.jsoup.nodes.Document;
            - URL : 크롤링할 웹 주소
            - 결과 : HTML 전체를 Document 객체로 가져옴
        (2) `Elements elements = document.select("CSS 선택자");`
            `Element element = document.selectFirst("CSS 선택자")`
            - CSS 선택자를 Element / Elements 타입으로 반환
        (3) `String title = element.text();`
            - 마크업 요소 내의 데이터 가져오기
        (4) `String attr = element.attr("속성명")`
            - 마크업 속성값 가져오기

[ Selenium + Chrome ]
    - 크롬을 활용한 눈속임으로 크롤링 함
    1) 설치 from mvn
        - Selenium java
            `implementation("org.seleniumhq.selenium:selenium-java:4.35.0")`
        - WebDriverManager
            https://mvnrepository.com/artifact/io.github.bonigarcia/webdrivermanager
            `implementation("io.github.bonigarcia:webdrivermanager:6.3.1")`